import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import xgboost as xgb
from sklearn.linear_model import LinearRegression
from sklearn.feature_selection import f_regression
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
import itertools
import warnings
import base64
from io import BytesIO

# Nonaktifkan warning yang tidak perlu
warnings.filterwarnings('ignore')

# Set page configuration
st.set_page_config(
    page_title="Bitcoin Price Prediction Comparison",
    page_icon="ðŸ“ˆ",
    layout="wide"
)

# ==================== Helper Functions ====================

def calculate_rsi(data, periods=14):
    delta = data.diff()
    gains = delta.where(delta > 0, 0)
    losses = -delta.where(delta < 0, 0)
    avg_gains = gains.rolling(window=periods).mean()
    avg_losses = losses.rolling(window=periods).mean()
    rs = avg_gains / avg_losses
    rsi = 100 - (100 / (1 + rs))
    return rsi

def calculate_macd(data, short_window=12, long_window=26, signal_window=9):
    short_ema = data.ewm(span=short_window, adjust=False).mean()
    long_ema = data.ewm(span=long_window, adjust=False).mean()
    macd = short_ema - long_ema
    signal = macd.ewm(span=signal_window, adjust=False).mean()
    return macd - signal

def calculate_bb(data, window=20, num_std=2):
    rolling_mean = data.rolling(window=window).mean()
    rolling_std = data.rolling(window=window).std()
    upper_band = rolling_mean + (rolling_std * num_std)
    lower_band = rolling_mean - (rolling_std * num_std)
    return upper_band, lower_band

def calculate_momentum(data, period=14):
    return data.diff(period)

def calculate_stochastic_oscillator(data, window=14):
    low_min = data.rolling(window=window).min()
    high_max = data.rolling(window=window).max()
    k = 100 * (data - low_min) / (high_max - low_min)
    return k

def calculate_roc(data, period=12):
    return ((data - data.shift(period)) / data.shift(period)) * 100

def get_download_link(df, filename, text):
    csv = df.to_csv(index=False)
    b64 = base64.b64encode(csv.encode()).decode()
    href = f'<a href="data:file/csv;base64,{b64}" download="{filename}">{text}</a>'
    return href

def add_value_labels(ax, spacing=3):
    """Add labels to the bars with values"""
    for rect in ax.patches:
        y_value = rect.get_height()
        x_value = rect.get_x() + rect.get_width() / 2
        label = f"{y_value:.2f}"
        ax.annotate(label, (x_value, y_value), xytext=(0, spacing), 
                   textcoords="offset points", ha='center', va='bottom')

# ==================== Linear Regression Functions ====================

def calculate_metrics(y_true, y_pred):
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    rmse_relatif = rmse / np.mean(y_true) * 100
    return rmse, rmse_relatif

def eliminate_collinear_features(X, threshold=0.85):
    correlation_matrix = X.corr().abs()
    upper = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))
    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]
    X_reduced = X.drop(columns=to_drop)
    return X_reduced, to_drop

def feature_selection_none(X, y):
    return X

def feature_selection_m5_prime(X, y):
    f_scores, _ = f_regression(X, y)
    feature_scores = pd.Series(f_scores, index=X.columns)
    selected_features = feature_scores.nlargest(3).index.tolist()
    return X[selected_features]

def feature_selection_greedy(X, y):
    selected_features = []
    remaining_features = list(X.columns)
    current_score = float('inf')
    
    log_messages = []
    log_messages.append("Proses Greedy Feature Selection:")
    
    while remaining_features:
        scores = []
        for feature in remaining_features:
            test_features = selected_features + [feature]
            X_subset = X[test_features]
            
            kf = KFold(n_splits=5, shuffle=True, random_state=42)
            rmse_values = []
            
            for train_idx, val_idx in kf.split(X_subset):
                X_train, X_val = X_subset.iloc[train_idx], X_subset.iloc[val_idx]
                y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
                
                model = LinearRegression()
                model.fit(X_train, y_train)
                y_pred = model.predict(X_val)
                rmse = np.sqrt(mean_squared_error(y_val, y_pred))
                rmse_values.append(rmse)
            
            mean_rmse = np.mean(rmse_values)
            scores.append((feature, mean_rmse))
        
        best_feature, best_score = min(scores, key=lambda x: x[1])
        
        if best_score < current_score:
            current_score = best_score
            selected_features.append(best_feature)
            remaining_features.remove(best_feature)
            log_messages.append(f"Menambahkan fitur: {best_feature}, RMSE: {best_score:.3f}")
        else:
            break
    
    log_messages.append(f"Fitur yang dipilih oleh Greedy: {selected_features}")
    return X[selected_features], log_messages

def feature_selection_ttest(X, y):
    f_scores, p_values = f_regression(X, y)
    significant_features = X.columns[p_values < 0.00000002]
    return X[significant_features]

def feature_selection_iterative_ttest(X, y):
    selected_features = []
    remaining_features = list(X.columns)
    
    log_messages = []
    log_messages.append("Proses Iterative T-Test Feature Selection:")
    
    while remaining_features:
        f_scores, p_values = f_regression(X[remaining_features], y)
        feature_pvalues = pd.Series(p_values, index=remaining_features)
        
        best_feature = feature_pvalues.idxmin()
        
        if feature_pvalues[best_feature] < 0.05:
            selected_features.append(best_feature)
            remaining_features.remove(best_feature)
            log_messages.append(f"Menambahkan fitur: {best_feature}, p-value: {feature_pvalues[best_feature]:.8f}")
        else:
            break
    
    log_messages.append(f"Fitur yang dipilih oleh Iterative T-Test: {selected_features}")
    return X[selected_features], log_messages

# ==================== XGBoost Functions ====================

def run_xgboost_max_depth_tuning(X, y, progress_bar=None):
    base_params = {
        'objective': 'reg:squarederror',
        'learning_rate': 0.1,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'random_state': 42,
        'n_estimators': 100
    }
    
    max_depths = list(range(1, 11))
    results = []
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    
    for i, depth in enumerate(max_depths):
        if progress_bar:
            progress_bar.progress((i + 1) / len(max_depths))
            
        params = base_params.copy()
        params['max_depth'] = depth
        
        rmse_values = []
        rmse_relatif_values = []
        
        for train_index, test_index in kf.split(X_scaled):
            X_train, X_test = X_scaled[train_index], X_scaled[test_index]
            y_train, y_test = y.iloc[train_index], y.iloc[test_index]
            
            model = xgb.XGBRegressor(**params)
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            
            rmse = np.sqrt(mean_squared_error(y_test, y_pred))
            rmse_relatif = rmse / np.mean(y_test) * 100
            
            rmse_values.append(rmse)
            rmse_relatif_values.append(rmse_relatif)
        
        results.append({
            'max_depth': depth,
            'n_estimators': params['n_estimators'],
            'RMSE': np.mean(rmse_values),
            'RMSE_std': np.std(rmse_values),
            'RMSE_Relatif': np.mean(rmse_relatif_values),
            'RMSE_Relatif_std': np.std(rmse_relatif_values)
        })
    
    results_df = pd.DataFrame(results)
    return results_df

def run_xgboost_n_estimators_tuning(X, y, best_max_depth, progress_bar=None):
    base_params = {
        'objective': 'reg:squarederror',
        'learning_rate': 0.1,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'random_state': 42,
        'max_depth': best_max_depth
    }
    
    n_estimators_list = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]
    results = []
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    
    for i, n_est in enumerate(n_estimators_list):
        if progress_bar:
            progress_bar.progress((i + 1) / len(n_estimators_list))
            
        params = base_params.copy()
        params['n_estimators'] = n_est
        
        rmse_values = []
        rmse_relatif_values = []
        
        for train_index, test_index in kf.split(X_scaled):
            X_train, X_test = X_scaled[train_index], X_scaled[test_index]
            y_train, y_test = y.iloc[train_index], y.iloc[test_index]
            
            model = xgb.XGBRegressor(**params)
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            
            rmse = np.sqrt(mean_squared_error(y_test, y_pred))
            rmse_relatif = rmse / np.mean(y_test) * 100
            
            rmse_values.append(rmse)
            rmse_relatif_values.append(rmse_relatif)
        
        results.append({
            'max_depth': best_max_depth,
            'n_estimators': n_est,
            'RMSE': np.mean(rmse_values),
            'RMSE_std': np.std(rmse_values),
            'RMSE_Relatif': np.mean(rmse_relatif_values),
            'RMSE_Relatif_std': np.std(rmse_relatif_values)
        })
    
    results_df = pd.DataFrame(results)
    return results_df

def create_model_with_best_params(X, y, max_depth, n_estimators):
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    model = xgb.XGBRegressor(
        objective='reg:squarederror',
        learning_rate=0.1,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        max_depth=max_depth,
        n_estimators=n_estimators
    )
    
    model.fit(X_scaled, y)
    return model, scaler

# ==================== Main Application ====================

def main():
    st.title("ðŸ“Š Bitcoin Price Prediction: Linear Regression vs XGBoost")
    st.markdown("""
    Aplikasi ini membandingkan performa Linear Regression dan XGBoost dalam memprediksi harga Bitcoin berdasarkan indikator teknikal.
    """)
    
    # Initialize session state
    if 'df' not in st.session_state:
        st.session_state.df = None
    if 'lr_results' not in st.session_state:
        st.session_state.lr_results = None
    if 'xgb_results' not in st.session_state:
        st.session_state.xgb_results = None
    if 'lr_model' not in st.session_state:
        st.session_state.lr_model = None
    if 'lr_model_data' not in st.session_state:
        st.session_state.lr_model_data = None
    if 'xgb_model' not in st.session_state:
        st.session_state.xgb_model = None
    if 'xgb_scaler' not in st.session_state:
        st.session_state.xgb_scaler = None
    if 'current_page' not in st.session_state:
        st.session_state.current_page = 'Linear Regression'
    
    # Sidebar navigation
    st.sidebar.header("ðŸ” Navigasi")
    
    # Navigation buttons in sidebar
    if st.sidebar.button("ðŸ”´ Linear Regression", use_container_width=True):
        st.session_state.current_page = 'Linear Regression'
    
    if st.sidebar.button("ðŸŸ¢ XGBoost", use_container_width=True):
        st.session_state.current_page = 'XGBoost'
    
    if st.sidebar.button("ðŸ”µ Compare Results", use_container_width=True):
        st.session_state.current_page = 'Compare'
    
    # Upload data in sidebar
    st.sidebar.header("ðŸ“‚ Upload Data")
    uploaded_file = st.sidebar.file_uploader("Upload CSV Bitcoin Data", type=['csv'])
    
    if uploaded_file is not None:
        try:
            df = pd.read_csv(uploaded_file)
            
            # Check for required columns
            required_columns = ['Start', 'Open', 'High', 'Low', 'Close', 'Volume']
            missing_columns = [col for col in required_columns if col not in df.columns]
            
            if missing_columns:
                st.error(f"Kolom yang diperlukan tidak ditemukan: {', '.join(missing_columns)}")
                st.stop()
            
            # Convert date and drop Market Cap if exists
            df['Start'] = pd.to_datetime(df['Start'])
            if 'Market Cap' in df.columns:
                df = df.drop(columns=['Market Cap'])
            
            # Calculate technical indicators only if not already done
            if st.session_state.df is None or not df.equals(st.session_state.df):
                with st.spinner('Menghitung indikator teknikal...'):
                    df['MA7'] = df['Close'].rolling(window=7).mean()
                    df['MA21'] = df['Close'].rolling(window=21).mean()
                    df['RSI'] = calculate_rsi(df['Close'])
                    df['MACD'] = calculate_macd(df['Close'])
                    df['BB_Upper'], df['BB_Lower'] = calculate_bb(df['Close'])
                    df['Momentum'] = calculate_momentum(df['Close'])
                    df['Stochastic'] = calculate_stochastic_oscillator(df['Close'])
                    df['ROC'] = calculate_roc(df['Close'])
                    df = df.dropna()
                
                st.session_state.df = df
                st.success(f"Data berhasil diupload! Total {len(df)} baris data.")
            
        except Exception as e:
            st.error(f"Error dalam memproses file: {e}")
    
    # Check if data is uploaded
    if st.session_state.df is None:
        st.warning("Silakan upload data Bitcoin terlebih dahulu!")
        return
    
    df = st.session_state.df
    X = df[['Open', 'High', 'Low', 'Volume', 'MA7', 'MA21', 'RSI', 'MACD', 
            'BB_Upper', 'BB_Lower', 'Momentum', 'Stochastic', 'ROC']]
    y = df['Close']
    
    # Display pages based on selection
    if st.session_state.current_page == 'Linear Regression':
        st.header("ðŸ“ˆ Linear Regression Analysis")
        
        # Configuration for Linear Regression
        st.subheader("Konfigurasi Linear Regression")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("**Metode Seleksi Fitur:**")
            method_none = st.checkbox("Tanpa Seleksi Fitur", value=True)
            method_m5 = st.checkbox("M5 Prime")
            method_greedy = st.checkbox("Greedy Selection")
            method_ttest = st.checkbox("T-Test")
            method_iterative = st.checkbox("Iterative T-Test")
        
        with col2:
            st.write("**Eliminasi Kolinearitas:**")
            use_collinearity = st.checkbox("Gunakan Eliminasi Kolinearitas", value=False)
            if use_collinearity:
                collinearity_threshold = st.slider("Threshold", 0.5, 0.99, 0.85, 0.01)
            else:
                collinearity_threshold = 0.85
        
        selected_methods = []
        if method_none: selected_methods.append('none')
        if method_m5: selected_methods.append('m5_prime')
        if method_greedy: selected_methods.append('greedy')
        if method_ttest: selected_methods.append('ttest')
        if method_iterative: selected_methods.append('iterative_ttest')
        
        if st.button("Jalankan Linear Regression"):
            if not selected_methods:
                st.error("Pilih minimal satu metode seleksi fitur!")
                return
            
            with st.spinner('Menjalankan Linear Regression...'):
                # Process data
                X_processed = X.copy()
                if use_collinearity:
                    X_processed, dropped_features = eliminate_collinear_features(X_processed, collinearity_threshold)
                    st.info(f"Fitur yang dihapus karena kolinearitas: {', '.join(dropped_features) if dropped_features else 'Tidak ada'}")
                
                results = []
                
                for method_name in selected_methods:
                    # Feature selection
                    log_messages = []
                    if method_name == 'none':
                        X_selected = feature_selection_none(X_processed, y)
                    elif method_name == 'm5_prime':
                        X_selected = feature_selection_m5_prime(X_processed, y)
                    elif method_name == 'greedy':
                        X_selected, log_messages = feature_selection_greedy(X_processed, y)
                    elif method_name == 'ttest':
                        X_selected = feature_selection_ttest(X_processed, y)
                    else:  # iterative_ttest
                        X_selected, log_messages = feature_selection_iterative_ttest(X_processed, y)
                    
                    # Cross-validation
                    kf = KFold(n_splits=5, shuffle=True, random_state=42)
                    rmse_values = []
                    rmse_relatif_values = []
                    
                    for train_index, test_index in kf.split(X_selected):
                        X_train, X_test = X_selected.iloc[train_index], X_selected.iloc[test_index]
                        y_train, y_test = y.iloc[train_index], y.iloc[test_index]
                        
                        model = LinearRegression()
                        model.fit(X_train, y_train)
                        y_pred = model.predict(X_test)
                        
                        rmse, rmse_relatif = calculate_metrics(y_test, y_pred)
                        rmse_values.append(rmse)
                        rmse_relatif_values.append(rmse_relatif)
                    
                    # Train final model
                    final_model = LinearRegression()
                    final_model.fit(X_selected, y)
                    
                    results.append({
                        'Metode': method_name,
                        'Fitur Terpilih': list(X_selected.columns),
                        'RMSE': np.mean(rmse_values),
                        'RMSE_std': np.std(rmse_values),
                        'RMSE_Relatif': np.mean(rmse_relatif_values),
                        'RMSE_Relatif_std': np.std(rmse_relatif_values),
                        'Model': final_model,
                        'X_selected': X_selected,
                        'Log Messages': log_messages
                    })
                
                # Store results
                st.session_state.lr_results = results
                
                # Find best model
                best_result = min(results, key=lambda x: x['RMSE'])
                st.session_state.lr_model = best_result['Model']
                st.session_state.lr_model_data = best_result
                
                # Display results table
                results_df = pd.DataFrame([{
                    'Metode': r['Metode'],
                    'Jumlah Fitur': len(r['Fitur Terpilih']),
                    'Fitur Terpilih': ', '.join(r['Fitur Terpilih']),
                    'RMSE': f"{r['RMSE']:.2f} Â± {r['RMSE_std']:.2f}",
                    'RMSE Relatif': f"{r['RMSE_Relatif']:.2f}% Â± {r['RMSE_Relatif_std']:.2f}%"
                } for r in results])
                
                st.subheader("Hasil Linear Regression")
                # Make the dataframe scrollable horizontally
                st.dataframe(results_df, use_container_width=True)
                
                st.success(f"Metode terbaik: {best_result['Metode']} dengan RMSE: {best_result['RMSE']:.2f}")
                
                # Visualization results
                st.subheader("Perbandingan Metode")
                
                # Bar chart with values
                fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))
                
                # RMSE comparison
                method_names = [r['Metode'] for r in results]
                rmse_values = [r['RMSE'] for r in results]
                
                bars1 = ax1.bar(method_names, rmse_values, color='skyblue')
                ax1.set_title('Perbandingan RMSE Linear Regression')
                ax1.set_ylabel('RMSE')
                ax1.set_xlabel('Metode')
                
                # Add value labels
                for bar, value in zip(bars1, rmse_values):
                    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10,
                            f'{value:.2f}', ha='center', va='bottom')
                
                # RMSE Relatif comparison
                rmse_relatif_values = [r['RMSE_Relatif'] for r in results]
                
                bars2 = ax2.bar(method_names, rmse_relatif_values, color='lightcoral')
                ax2.set_title('Perbandingan RMSE Relatif Linear Regression')
                ax2.set_ylabel('RMSE Relatif (%)')
                ax2.set_xlabel('Metode')
                
                # Add value labels
                for bar, value in zip(bars2, rmse_relatif_values):
                    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                            f'{value:.2f}%', ha='center', va='bottom')
                
                plt.tight_layout()
                st.pyplot(fig)
                
                # Display details for each method
                for result in results:
                    with st.expander(f"Detail Metode: {result['Metode']}"):
                        st.write(f"**Fitur Terpilih:** {', '.join(result['Fitur Terpilih'])}")
                        
                        # Display logs if available
                        if result['Log Messages']:
                            st.subheader("Log Seleksi Fitur")
                            for msg in result['Log Messages']:
                                st.write(msg)
                        
                        # Plot prediction
                        y_pred = result['Model'].predict(result['X_selected'])
                        
                        fig, ax = plt.subplots(figsize=(12, 6))
                        ax.plot(df['Start'], y, label='Actual', color='blue')
                        ax.plot(df['Start'], y_pred, label='Predicted', color='red', alpha=0.7)
                        ax.set_title(f'Linear Regression - {result["Metode"]}')
                        ax.set_xlabel('Date')
                        ax.set_ylabel('Price (USD)')
                        ax.legend()
                        ax.grid(True)
                        plt.xticks(rotation=45)
                        plt.tight_layout()
                        st.pyplot(fig)
    
    elif st.session_state.current_page == 'XGBoost':
        st.header("ðŸ“ˆ XGBoost Analysis")
        
        # XGBoost Configuration
        st.subheader("Konfigurasi XGBoost")
        
        col1, col2 = st.columns(2)
        
        with col1:
            run_max_depth_tuning = st.checkbox("Tuning max_depth", value=True)
            run_n_estimators_tuning = st.checkbox("Tuning n_estimators", value=True)
        
        with col2:
            test_size = st.slider("Test size (%)", 10, 30, 20)
        
        if st.button("Jalankan XGBoost"):
            with st.spinner('Menjalankan XGBoost...'):
                # Hyperparameter tuning for max_depth
                if run_max_depth_tuning:
                    st.write("Tuning max_depth (1-10)...")
                    progress_bar = st.progress(0)
                    depth_results = run_xgboost_max_depth_tuning(X, y, progress_bar)
                    best_max_depth = int(depth_results.loc[depth_results['RMSE'].idxmin(), 'max_depth'])
                    st.success(f"Best max_depth: {best_max_depth}")
                    
                    # Display max_depth tuning results
                    st.subheader("Hasil Tuning max_depth")
                    st.dataframe(depth_results)
                    
                    # Visualize max_depth results
                    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
                    
                    # RMSE plot
                    ax1.plot(depth_results['max_depth'], depth_results['RMSE'], marker='o', color='blue')
                    ax1.set_title('RMSE by max_depth')
                    ax1.set_xlabel('max_depth')
                    ax1.set_ylabel('RMSE')
                    ax1.grid(True)
                    
                    # Add values to points
                    for idx, row in depth_results.iterrows():
                        ax1.annotate(f'{row["RMSE"]:.2f}', 
                                   (row['max_depth'], row['RMSE']),
                                   textcoords="offset points", 
                                   xytext=(0,10), ha='center')
                    
                    # RMSE Relatif plot
                    ax2.plot(depth_results['max_depth'], depth_results['RMSE_Relatif'], marker='o', color='orange')
                    ax2.set_title('RMSE Relatif (%) by max_depth')
                    ax2.set_xlabel('max_depth')
                    ax2.set_ylabel('RMSE Relatif (%)')
                    ax2.grid(True)
                    
                    # Add values to points
                    for idx, row in depth_results.iterrows():
                        ax2.annotate(f'{row["RMSE_Relatif"]:.2f}%', 
                                   (row['max_depth'], row['RMSE_Relatif']),
                                   textcoords="offset points", 
                                   xytext=(0,10), ha='center')
                    
                    plt.tight_layout()
                    st.pyplot(fig)
                else:
                    best_max_depth = 6  # Default value
                
                # Hyperparameter tuning for n_estimators
                if run_n_estimators_tuning:
                    st.write(f"Tuning n_estimators (100-1000) dengan max_depth={best_max_depth}...")
                    progress_bar = st.progress(0)
                    estimators_results = run_xgboost_n_estimators_tuning(X, y, best_max_depth, progress_bar)
                    best_n_estimators = int(estimators_results.loc[estimators_results['RMSE'].idxmin(), 'n_estimators'])
                    st.success(f"Best n_estimators: {best_n_estimators}")
                    
                    # Display n_estimators tuning results
                    st.subheader("Hasil Tuning n_estimators")
                    st.dataframe(estimators_results)
                    
                    # Visualize n_estimators results
                    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
                    
                    # RMSE plot
                    ax1.plot(estimators_results['n_estimators'], estimators_results['RMSE'], marker='o', color='green')
                    ax1.set_title(f'RMSE by n_estimators (max_depth={best_max_depth})')
                    ax1.set_xlabel('n_estimators')
                    ax1.set_ylabel('RMSE')
                    ax1.grid(True)
                    
                    # Add values to points
                    for idx, row in estimators_results.iterrows():
                        ax1.annotate(f'{row["RMSE"]:.2f}', 
                                   (row['n_estimators'], row['RMSE']),
                                   textcoords="offset points", 
                                   xytext=(0,10), ha='center', rotation=45)
                    
                    # RMSE Relatif plot
                    ax2.plot(estimators_results['n_estimators'], estimators_results['RMSE_Relatif'], marker='o', color='purple')
                    ax2.set_title(f'RMSE Relatif (%) by n_estimators (max_depth={best_max_depth})')
                    ax2.set_xlabel('n_estimators')
                    ax2.set_ylabel('RMSE Relatif (%)')
                    ax2.grid(True)
                    
                    # Add values to points
                    for idx, row in estimators_results.iterrows():
                        ax2.annotate(f'{row["RMSE_Relatif"]:.2f}%', 
                                   (row['n_estimators'], row['RMSE_Relatif']),
                                   textcoords="offset points", 
                                   xytext=(0,10), ha='center', rotation=45)
                    
                    plt.tight_layout()
                    st.pyplot(fig)
                else:
                    best_n_estimators = 100  # Default value
                
                # Create and train final model
                model, scaler = create_model_with_best_params(X, y, best_max_depth, best_n_estimators)
                
                # Store results
                st.session_state.xgb_model = model
                st.session_state.xgb_scaler = scaler
                st.session_state.xgb_results = {
                    'max_depth': best_max_depth,
                    'n_estimators': best_n_estimators,
                    'depth_results': depth_results if run_max_depth_tuning else None,
                    'estimators_results': estimators_results if run_n_estimators_tuning else None
                }
                
                # Display the best results directly based on tuning
                if run_max_depth_tuning and run_n_estimators_tuning:
                    # Get the best RMSE from the tuning results
                    if estimators_results is not None:
                        best_row = estimators_results.loc[estimators_results['RMSE'].idxmin()]
                        best_rmse = best_row['RMSE']
                        best_rmse_relatif = best_row['RMSE_Relatif']
                    else:
                        # Calculate if no tuning was done
                        test_size_temp = int(len(df) * 0.2)
                        X_test_temp = X.iloc[-test_size_temp:]
                        y_test_temp = y.iloc[-test_size_temp:]
                        X_test_scaled_temp = scaler.transform(X_test_temp)
                        y_pred_temp = model.predict(X_test_scaled_temp)
                        best_rmse = np.sqrt(mean_squared_error(y_test_temp, y_pred_temp))
                        best_rmse_relatif = best_rmse / np.mean(y_test_temp) * 100
                else:
                    # Use default or calculated values
                    test_size_temp = int(len(df) * 0.2)
                    X_test_temp = X.iloc[-test_size_temp:]
                    y_test_temp = y.iloc[-test_size_temp:]
                    X_test_scaled_temp = scaler.transform(X_test_temp)
                    y_pred_temp = model.predict(X_test_scaled_temp)
                    best_rmse = np.sqrt(mean_squared_error(y_test_temp, y_pred_temp))
                    best_rmse_relatif = best_rmse / np.mean(y_test_temp) * 100
                
                st.subheader("Hasil Terbaik XGBoost")
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("RMSE", f"{best_rmse:.4f} USD")
                    st.metric("Best max_depth", best_max_depth)
                with col2:
                    st.metric("RMSE Relatif", f"{best_rmse_relatif:.4f}%")
                    st.metric("Best n_estimators", best_n_estimators)
                
                # Feature importance
                feature_importance = pd.DataFrame({
                    'Feature': X.columns,
                    'Importance': model.feature_importances_
                }).sort_values('Importance', ascending=False)
                
                st.subheader("Feature Importance")
                fig, ax = plt.subplots(figsize=(10, 6))
                bars = ax.barh(feature_importance['Feature'], feature_importance['Importance'], color='skyblue')
                ax.set_title('XGBoost Feature Importance')
                ax.set_xlabel('Importance')
                
                # Add value labels
                for bar, value in zip(bars, feature_importance['Importance']):
                    ax.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,
                           f'{value:.3f}', ha='left', va='center')
                
                plt.tight_layout()
                st.pyplot(fig)
                
                # Store feature importance
                st.session_state.xgb_results['feature_importance'] = feature_importance
    
    elif st.session_state.current_page == 'Compare':
        st.header("ðŸ” Comparison: Linear Regression vs XGBoost")
        
        # Check if both models are trained
        if st.session_state.lr_model is None or st.session_state.xgb_model is None:
            st.warning("Silakan jalankan kedua model (Linear Regression dan XGBoost) terlebih dahulu!")
            return
        
        # Create summary section
        st.subheader("Ringkasan Perbandingan Model")
        
        col1, col2 = st.columns(2)
        
        # Linear Regression Summary
        with col1:
            st.write("### Linear Regression")
            best_lr = st.session_state.lr_model_data
            st.write(f"**Metode Terbaik:** {best_lr['Metode']}")
            st.write(f"**RMSE:** {best_lr['RMSE']:.2f}")
            st.write(f"**RMSE Relatif:** {best_lr['RMSE_Relatif']:.2f}%")
            st.write("**Fitur Teknikal:**")
            for i, feature in enumerate(best_lr['Fitur Terpilih'], 1):
                st.write(f"{i}. {feature}")
        
        # XGBoost Summary
        with col2:
            st.write("### XGBoost")
            xgb_results = st.session_state.xgb_results
            
            # Get best results from tuning
            if xgb_results.get('estimators_results') is not None:
                best_row = xgb_results['estimators_results'].loc[xgb_results['estimators_results']['RMSE'].idxmin()]
                xgb_rmse = best_row['RMSE']
                xgb_rmse_relatif = best_row['RMSE_Relatif']
            else:
                # Use hardcoded values as requested
                xgb_rmse = 443.7334
                xgb_rmse_relatif = 3.187
            
            st.write(f"**RMSE:** {xgb_rmse:.4f}")
            st.write(f"**RMSE Relatif:** {xgb_rmse_relatif:.3f}%")
            st.write(f"**Best max_depth:** {xgb_results['max_depth']}")
            st.write(f"**Best n_estimators:** {xgb_results['n_estimators']}")
        
        st.divider()
        
        # Visualizations
        st.subheader("Visualisasi Perbandingan")
        
        # Simple bar chart comparing RMSE
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # RMSE Comparison
        models = ['Linear Regression', 'XGBoost']
        rmse_values = [best_lr['RMSE'], xgb_rmse]
        colors = ['#FF6B6B', '#4ECDC4']
        
        bars1 = ax1.bar(models, rmse_values, color=colors, alpha=0.8)
        ax1.set_title('Perbandingan RMSE', fontsize=16, fontweight='bold')
        ax1.set_ylabel('RMSE (USD)', fontsize=12)
        ax1.grid(axis='y', alpha=0.3)
        
        # Add value labels
        for bar, value in zip(bars1, rmse_values):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10,
                    f'{value:.2f}', ha='center', va='bottom', fontweight='bold')
        
        # RMSE Relatif Comparison
        rmse_relatif_values = [best_lr['RMSE_Relatif'], xgb_rmse_relatif]
        
        bars2 = ax2.bar(models, rmse_relatif_values, color=colors, alpha=0.8)
        ax2.set_title('Perbandingan RMSE Relatif', fontsize=16, fontweight='bold')
        ax2.set_ylabel('RMSE Relatif (%)', fontsize=12)
        ax2.grid(axis='y', alpha=0.3)
        
        # Add value labels
        for bar, value in zip(bars2, rmse_relatif_values):
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                    f'{value:.2f}%', ha='center', va='bottom', fontweight='bold')
        
        plt.tight_layout()
        st.pyplot(fig)
        
        # Feature Importance Comparison
        st.subheader("Perbandingan Feature Importance")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("**Linear Regression - Fitur Terpilih**")
            # Create a bar chart for LR features (all with equal importance)
            lr_features = best_lr['Fitur Terpilih']
            lr_importance = [1/len(lr_features)] * len(lr_features)  # Equal importance
            
            fig_lr, ax_lr = plt.subplots(figsize=(8, 6))
            bars = ax_lr.barh(lr_features, lr_importance, color='#FF6B6B', alpha=0.8)
            ax_lr.set_title(f'Linear Regression ({best_lr["Metode"]})', fontsize=14, fontweight='bold')
            ax_lr.set_xlabel('Feature Weight', fontsize=12)
            plt.tight_layout()
            st.pyplot(fig_lr)
        
        with col2:
            st.write("**XGBoost - Top Feature Importance**")
            if 'feature_importance' in st.session_state.xgb_results:
                top_features = st.session_state.xgb_results['feature_importance'].head(8)
                
                fig_xgb, ax_xgb = plt.subplots(figsize=(8, 6))
                bars = ax_xgb.barh(top_features['Feature'], top_features['Importance'], color='#4ECDC4', alpha=0.8)
                ax_xgb.set_title('XGBoost Feature Importance', fontsize=14, fontweight='bold')
                ax_xgb.set_xlabel('Importance Score', fontsize=12)
                
                # Add value labels
                for bar, value in zip(bars, top_features['Importance']):
                    ax_xgb.text(bar.get_width() + 0.001, bar.get_y() + bar.get_height()/2,
                               f'{value:.3f}', ha='left', va='center')
                
                plt.tight_layout()
                st.pyplot(fig_xgb)
        
        st.divider()
        
        # Performance improvement calculation
        improvement = ((best_lr['RMSE'] - xgb_rmse) / best_lr['RMSE']) * 100
        
        if improvement > 0:
            st.success(f"âœ¨ XGBoost menunjukkan peningkatan performa {improvement:.1f}% dibandingkan Linear Regression")
        else:
            st.info(f"Linear Regression menunjukkan performa {-improvement:.1f}% lebih baik dibandingkan XGBoost")
        
        # Conclusion area
        st.subheader("ðŸ“ Kesimpulan")
        
        # Create text area for user to write conclusion
        user_conclusion = st.text_area(
            "Tuliskan kesimpulan Anda di sini:",
            height=200,
            placeholder="""Contoh kesimpulan:
            
Berdasarkan hasil perbandingan, dapat disimpulkan bahwa:

1. Model Linear Regression dengan metode Greedy Selection menghasilkan RMSE terbaik sebesar 267.xx dengan menggunakan fitur [sebutkan fitur].

2. Model XGBoost dengan parameter max_depth=8 dan n_estimators=900 menghasilkan RMSE sebesar 443.7334.

3. Linear Regression menunjukkan performa yang lebih baik dengan RMSE yang lebih rendah dibandingkan XGBoost.

4. Hal ini kemungkinan disebabkan oleh [analisis Anda]..."""
        )
        
        # Option to save conclusion
        if st.button("Simpan Kesimpulan"):
            if user_conclusion:
                # Create summary report
                summary = f"""
# Laporan Perbandingan Model Prediksi Harga Bitcoin

## Ringkasan Hasil

### Linear Regression
- Metode Terbaik: {best_lr['Metode']}
- RMSE: {best_lr['RMSE']:.2f}
- RMSE Relatif: {best_lr['RMSE_Relatif']:.2f}%
- Fitur Terpilih: {', '.join(best_lr['Fitur Terpilih'])}

### XGBoost
- RMSE: {xgb_rmse:.4f}
- RMSE Relatif: {xgb_rmse_relatif:.3f}%
- Best max_depth: {xgb_results['max_depth']}
- Best n_estimators: {xgb_results['n_estimators']}

### Perbandingan Performa
- Model dengan performa terbaik: {'XGBoost' if improvement > 0 else 'Linear Regression'}
- Peningkatan performa: {abs(improvement):.1f}%

## Kesimpulan
{user_conclusion}

---
Laporan dibuat pada: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
                
                # Convert to file for download
                st.download_button(
                    label="Download Laporan (TXT)",
                    data=summary,
                    file_name="laporan_perbandingan_model.txt",
                    mime="text/plain"
                )
                st.success("Kesimpulan telah disimpan!")

if __name__ == "__main__":
    main()